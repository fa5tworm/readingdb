<!DOCTYPE html>
	<html lang='en'>
	<head>
		<meta charset='UTF-8'>
		<meta name='viewport' content='width=device-width, initial-scale=1.0'>
		<meta name="description" content="Unlock your English reading skills: Immerse yourself in a contextual and practical learning enhanced by high-quality instant translations.">
      	<meta name="keywords" content="English reading, English language learning, instant translations, reading adventure, improve English, improve reading skills, educational platform, English reading resources, online education, English proficiency, learn English, English learning resources, bilingual education, language exchange, study English online">
		<link rel="apple-touch-icon" sizes="180x180" href="../../../static/favicons/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../../../static/favicons/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../../../static/favicons/favicon-16x16.png">
		<link rel="manifest" href="../../../static/favicons/site.webmanifest">
		<title>Shane Legg and Chris Anderson : The transformative potential of AGI — and when it might arrive</title>
		<link rel='stylesheet' href='../../../static/styles/styles.css'>
		<script defer data-domain="readingdb.org" src="https://plausible.io/js/script.js"></script>
	</head>
	<body>

	<h1 class='title'>The transformative potential of AGI — and when it might arrive</h1>
	<h2 class='speaker'>Shane Legg and Chris Anderson</h2>
	<div class='transcript books'>
	<p>Are you ready to improve your Spanish speaking skills, check <a href='https://referral.lingoda.com/6Dztrv'>Lingoda</a> today!</p>
		<p>
		<span class="tooltip">Chris Anderson: Shane, 
danos una instantánea de tu infancia<span class="tooltiptext">Chris Anderson: Shane, give us
a snapshot of you growing up</span></span>
		<span class="tooltip">y ¿qué te llevó a interesarte
por la inteligencia artificial?<span class="tooltiptext">and what on Earth led you to get
interested in artificial intelligence?</span></span>
	</p>
	<p>
		<span class="tooltip">Shane Legg: Bueno, compré mi primer
ordenador en casa cuando cumplí 10 años,<span class="tooltiptext">Shane Legg: Well, I got my first
home computer on my 10th birthday,</span></span>
		<span class="tooltip">eso fue antes de Internet y todo eso.<span class="tooltiptext">and I --</span></span>
		<span class="tooltip">No podías simplemente navegar
por la web y esas cosas.<span class="tooltiptext">this was before the internet
and everything.</span></span>
		<span class="tooltip">De hecho, tenías que hacer
cosas tú mismo y programarlas.<span class="tooltiptext">So you couldn&#39;t just go
and surf the web and so on.</span></span>
		<span class="tooltip">Empecé a programar<span class="tooltiptext">You had to actually make stuff
yourself and program.</span></span>
		<span class="tooltip">y descubrí que en este ordenador
había un mundo, podía crear un mundo,<span class="tooltiptext">And so I started programming,</span></span>
		<span class="tooltip">podía crear pequeños agentes
que corrían por ahí<span class="tooltiptext">and I discovered that in this computer
there was a world,</span></span>
		<span class="tooltip">persiguiéndose unos a otros
y haciendo cosas, etc.<span class="tooltiptext">I could create a world,</span></span>
		<span class="tooltip">Y podía, en cierto modo,
dar vida a todo este universo.<span class="tooltiptext">I could create little agents
that would run around</span></span>
		<span class="tooltip">Y había una chispa de creatividad
que realmente me cautivó<span class="tooltiptext">and chase each other
and do things and so on.</span></span>
		<span class="tooltip">y creo que esa fue la semilla
de mi interés,<span class="tooltiptext">And I could sort of,
bring this whole universe to life.</span></span>
		<span class="tooltip">que luego se volvió un interés
por la inteligencia artificial.<span class="tooltiptext">And there was  sort of that spark
of creativity that really captivated me</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Porque en tu educación estándar
tenías algunos desafíos en ese sentido.<span class="tooltiptext">and sort of, I think
that was really the seeds</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí, de niño fui disléxico.<span class="tooltiptext">of my interest that later grew</span></span>
		<span class="tooltip">Me iban a hacer repetir un año<span class="tooltiptext">into an interest
in artificial intelligence.</span></span>
		<span class="tooltip">cuando tenía 10 años,<span class="tooltiptext">CA: Because in your standard education,
you had some challenges there.</span></span>
		<span class="tooltip">y me mandaron a hacer una prueba
de coeficiente intelectual<span class="tooltiptext">SL: Yeah, I was dyslexic as a child.</span></span>
		<span class="tooltip">para evaluar la gravedad del problema.<span class="tooltiptext">And so they were actually
going to hold me back a year</span></span>
		<span class="tooltip">Y descubrieron que tenía un CI
excepcionalmente alto.<span class="tooltiptext">when I was 10 years old,</span></span>
		<span class="tooltip">Estaban un poco confundidos
por lo que estaba pasando.<span class="tooltiptext">and they sent me off
to get my IQ tested to sort of,</span></span>
		<span class="tooltip">Por fortuna, en esa época,<span class="tooltiptext">you know, assess how bad the problem was.</span></span>
		<span class="tooltip">alguien en la ciudad donde vivía
aplicaba pruebas de dislexia.<span class="tooltiptext">And they discovered I had
an exceptionally high IQ.</span></span>
		<span class="tooltip">Y resulta que en realidad
no tenía una inteligencia limitada.<span class="tooltiptext">And then they were a little bit confused
about what was going on.</span></span>
		<span class="tooltip">Era disléxico, ese era el problema.<span class="tooltiptext">And fortunately, at that time,</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Tenías motivos desde 
temprana edad para creer<span class="tooltiptext">there was somebody in the town I lived in</span></span>
		<span class="tooltip">que algunas ideas sobre inteligencia 
artificial podían ser un poco erróneas<span class="tooltiptext">who knew how to test for dyslexia.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Bueno, desde muy temprana edad<span class="tooltiptext">And it turns out I wasn&#39;t actually
of limited intelligence.</span></span>
	</p>
	<p>
		<span class="tooltip">tuve motivos para dudar
a veces de la autoridad.<span class="tooltiptext">I was dyslexic, and that was the issue.</span></span>
	</p>
	<p>
		<span class="tooltip">(Risas)<span class="tooltiptext">CA: You had reason
from an early age to believe</span></span>
	</p>
	<p>
		<span class="tooltip">Si el profesor piensa que eres tonto,
tal vez no sea cierto.<span class="tooltiptext">that our standard assumptions
about intelligence might be off a bit.</span></span>
		<span class="tooltip">Quizá estén pasando otras cosas.<span class="tooltiptext">SL: Well, I had reason, from an early age,
to sometimes doubt authority.</span></span>
		<span class="tooltip">Pero creo que también despertó
en mí un interés por la inteligencia<span class="tooltiptext">(Laughter)</span></span>
		<span class="tooltip">cuando tuve esa experiencia
cuando era niño.<span class="tooltiptext">You know, if the teacher thinks
you&#39;re dumb, maybe it&#39;s not true.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Muchos te atribuyen
el haber acuñado el término<span class="tooltiptext">Maybe there are other things going on.</span></span>
		<span class="tooltip">«inteligencia artificial general», IAG.<span class="tooltiptext">But I think it also created in me
an interest in intelligence</span></span>
		<span class="tooltip">Háblenos de 2001 y de cómo ocurrió.<span class="tooltiptext">when I sort of had
that experience as a child.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí, alguien llamado Ben Goertzel,<span class="tooltiptext">CA: So you&#39;re credited by many</span></span>
		<span class="tooltip">con quien había estado trabajando,
se puso en contacto conmigo<span class="tooltiptext">as coining the term
“artificial general intelligence,” AGI.</span></span>
		<span class="tooltip">e iba a escribir un libro<span class="tooltiptext">Tell us about 2001, how that happened.</span></span>
		<span class="tooltip">y estaba pensando en un libro 
sobre sistemas de inteligencia artificial<span class="tooltiptext">SL: Yeah, so I was approached
by someone called Ben Goertzel</span></span>
		<span class="tooltip">que fuera mucho más general y eficaz,<span class="tooltiptext">who I&#39;d actually been working with,</span></span>
		<span class="tooltip">en lugar de centrarse
en cosas muy limitadas.<span class="tooltiptext">and he was going to write a book,</span></span>
		<span class="tooltip">Estaba pensando en ponerle
un título al libro.<span class="tooltiptext">and he was thinking
about a book on AI systems</span></span>
		<span class="tooltip">Así que le sugerí: «Si te interesan
los sistemas muy generales,<span class="tooltiptext">that would be much more
general and capable,</span></span>
		<span class="tooltip">llámalo inteligencia artificial general».<span class="tooltiptext">rather than focusing
on very narrow things.</span></span>
		<span class="tooltip">Así que se decantó por eso.<span class="tooltiptext">And he was thinking
about a title for the book.</span></span>
		<span class="tooltip">Luego él y otras personas empezaron
a usar el término en línea<span class="tooltiptext">So I suggested to him, &quot;If you&#39;re
interested in very general systems,</span></span>
		<span class="tooltip">e Internet,<span class="tooltiptext">call it artificial general intelligence.&quot;</span></span>
		<span class="tooltip">y después se popularizó
a partir de ahí.<span class="tooltiptext">And so he went with that.</span></span>
		<span class="tooltip">Más tarde descubrimos que Mike Garrod<span class="tooltiptext">And then him and various other people
started using the term online</span></span>
		<span class="tooltip">publicó un artículo en una revista
de nanotecnología de seguridad en 1997.<span class="tooltiptext">and the internet,</span></span>
		<span class="tooltip">De hecho, fue la primera
persona que usó el término.<span class="tooltiptext">and then it sort of became
popularized from there.</span></span>
		<span class="tooltip">Pero de todos modos quiso decir
más o menos lo mismo que nosotros.<span class="tooltiptext">We later discovered there was
someone called Mike Garrod,</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Era más o menos una idea
a la que le había llegado el momento<span class="tooltiptext">who published a paper in a security
nanotech journal in &#39;97.</span></span>
		<span class="tooltip">de reconocer el potencial que había aquí.<span class="tooltiptext">So he is actually the first person
to have used the term.</span></span>
		<span class="tooltip">Hiciste una predicción temprana que
mucha gente pensó que era descabellada.<span class="tooltiptext">But it turns out he pretty much meant
the same thing as us anyway.</span></span>
		<span class="tooltip">¿Qué era eso?<span class="tooltiptext">CA: It was kind of an idea
whose time had come,</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Bueno, aproximadamente en 2001,<span class="tooltiptext">to recognize the potential here.</span></span>
		<span class="tooltip">una época similar a la que sugerí utilizar
el término inteligencia artificial general<span class="tooltiptext">I mean, you made an early prediction
that many people thought was bonkers.</span></span>
		<span class="tooltip">leí un libro de Ray Kurzweil 
que se titulaba<span class="tooltiptext">What was that?</span></span>
		<span class="tooltip">«La era de las máquinas espirituales»,<span class="tooltiptext">SL: Well, in about 2001,</span></span>
		<span class="tooltip">y llegué a la conclusión 
de que tenía toda la razón:<span class="tooltiptext">a similar time to when I suggested
this term artificial general intelligence,</span></span>
		<span class="tooltip">que la computación probablemente 
crecería exponencialmente<span class="tooltiptext">I read a book by Ray Kurzweil, actually,
&quot;Age of Spiritual Machines,&quot;</span></span>
		<span class="tooltip">durante al menos unas décadas,<span class="tooltiptext">and I concluded that he was
fundamentally right,</span></span>
		<span class="tooltip">y la cantidad de datos en el mundo
crecería exponencialmente<span class="tooltiptext">that computation was likely to grow
exponentially for at least a few decades,</span></span>
		<span class="tooltip">durante algunas décadas.<span class="tooltiptext">and the amount of data in the world
would grow exponentially</span></span>
		<span class="tooltip">Así que pensé que si eso sucedía,<span class="tooltiptext">for a few decades.</span></span>
		<span class="tooltip">el valor de los algoritmos 
extremadamente escalables<span class="tooltiptext">And so I figured
that if that was going to happen,</span></span>
		<span class="tooltip">que pudieran aprovechar todos
estos datos y cálculos<span class="tooltiptext">then the value of extremely
scalable algorithms</span></span>
		<span class="tooltip">sería muy alto.<span class="tooltiptext">that could harness
all this data and computation</span></span>
	</p>
	<p>
		<span class="tooltip">También me imaginé que, a mediados
de la década de 2020,<span class="tooltiptext">were going to be very high.</span></span>
		<span class="tooltip">sería posible,<span class="tooltiptext">And then I also figured
that in the mid 2020s,</span></span>
		<span class="tooltip">si tuviéramos estos algoritmos 
altamente escalables,<span class="tooltiptext">it would be possible then,</span></span>
		<span class="tooltip">entrenar sistemas de IA
con muchos más datos<span class="tooltiptext">if we had these highly
scalable algorithms,</span></span>
		<span class="tooltip">de los que un humano 
podría obtener en toda su vida.<span class="tooltiptext">to train artificial intelligence systems</span></span>
		<span class="tooltip">Como resultado de ello,
lo pueden encontrar en mi blog<span class="tooltiptext">on far more data than a human
would experience in a lifetime.</span></span>
		<span class="tooltip">desde aproximadamente 2009.<span class="tooltiptext">And so as a result of that,</span></span>
		<span class="tooltip">Creo que es la primera vez que hablo
públicamente sobre ello.<span class="tooltiptext">you can find it on my blog
from about 2009</span></span>
		<span class="tooltip">Predije una probabilidad del 50 %
de que se produjera una IAG en 2028.<span class="tooltiptext">I think it&#39;s the first time
I publicly talked about it,</span></span>
		<span class="tooltip">Sigo creyendo eso hoy en día.<span class="tooltiptext">I predicted a 50 percent
chance of AGI by 2028.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Sigue siendo tu fecha.<span class="tooltiptext">I still believe that today.</span></span>
		<span class="tooltip">¿Cómo definías la IAG en aquel
entonces ? ¿Ha cambiado tu definición?<span class="tooltiptext">CA: That&#39;s still your date.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí, al principio no tenía 
una definición particularmente precisa.<span class="tooltiptext">How did you define AGI back then,
and has your definition changed?</span></span>
		<span class="tooltip">En realidad, era solo una idea de
sistemas que serían mucho más generales.<span class="tooltiptext">SL: Yeah, I didn&#39;t have a particularly
precise definition at the beginning.</span></span>
		<span class="tooltip">En lugar de simplemente jugar
al Go o al ajedrez o algo así,<span class="tooltiptext">It was really just an idea of systems
that would just be far more general.</span></span>
		<span class="tooltip">poder hacer muchísimas cosas diferentes.<span class="tooltiptext">So rather than just playing
Go or chess or something,</span></span>
		<span class="tooltip">La definición que uso ahora
es que es un sistema<span class="tooltiptext">rather than actually be able to do many,
many different things.</span></span>
		<span class="tooltip">que puede realizar todo
tipo de tareas cognitivas<span class="tooltiptext">The definition I use now
is that it&#39;s a system</span></span>
		<span class="tooltip">que las personas podemos realizar,
posiblemente más,<span class="tooltiptext">that can do all the cognitive
kinds of tasks</span></span>
		<span class="tooltip">pero al menos puede realizar
tareas cognitivas que<span class="tooltiptext">that people can do, possibly more,</span></span>
		<span class="tooltip">podemos realizar normalmente.<span class="tooltiptext">but at least it can do
the sorts of cognitive tasks</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Hablemos solo de
la fundación de DeepMind<span class="tooltiptext">that people can typically do.</span></span>
		<span class="tooltip">y de la interacción
entre tú y tus cofundadores.<span class="tooltiptext">CA: So talk about just
the founding of DeepMind</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Correcto. Fui a Londres,
a un lugar llamado Unidad Gatsby,<span class="tooltiptext">and the interplay between you
and your cofounders.</span></span>
		<span class="tooltip">que estudia neurociencia teórica
y aprendizaje automático.<span class="tooltiptext">SL: Right. So I went to London
to the place called the Gatsby Unit,</span></span>
		<span class="tooltip">Y me interesaba
aprender las relaciones<span class="tooltiptext">which studies theoretical neuroscience
and machine learning.</span></span>
		<span class="tooltip">entre lo que entendemos sobre el cerebro<span class="tooltiptext">And I was interested in learning
the relationships</span></span>
		<span class="tooltip">y lo que sabemos del 
aprendizaje automático.<span class="tooltiptext">between what we understand about the brain</span></span>
		<span class="tooltip">Me pareció un lugar muy bueno.<span class="tooltiptext">and what we know from machine learning.</span></span>
		<span class="tooltip">Allí conocí a Demis Hassabis.<span class="tooltiptext">So that seemed like a really good place.</span></span>
		<span class="tooltip">Tenía el mismo supervisor
de posdoctorado que yo,<span class="tooltiptext">And I met Demis Hassabis there.</span></span>
		<span class="tooltip">y empezamos a hablar.<span class="tooltiptext">He had the same postdoc supervisor as me,</span></span>
		<span class="tooltip">Y me convenció de que<span class="tooltiptext">and we got talking.</span></span>
		<span class="tooltip">era el momento de crear 
una empresa en ese momento.<span class="tooltiptext">And he convinced me</span></span>
		<span class="tooltip">Fue en 2009 cuando empezamos a hablar.<span class="tooltiptext">that it was the time
to start a company then.</span></span>
		<span class="tooltip">Yo era un poco escéptico.<span class="tooltiptext">That was in 2009 we started talking.</span></span>
		<span class="tooltip">Pensaba que la IAG estaba
todavía demasiado lejos,<span class="tooltiptext">And I was a little bit skeptical.</span></span>
		<span class="tooltip">pero él pensaba que era el momento
adecuado, así que decidimos hacerlo.<span class="tooltiptext">I thought AGI was still
a bit too far away,</span></span>
		<span class="tooltip">Y luego un amigo suyo
era Mustafa Suleyman.<span class="tooltiptext">but he thought the time was right,
so we decided to go for it.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Y específicamente, ¿uno de
los objetivos de la empresa<span class="tooltiptext">And then a friend of his
was Mustafa Suleyman.</span></span>
		<span class="tooltip">era encontrar un camino hacia la AGI?<span class="tooltiptext">CA: And specifically,
one of the goals of the company</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Absolutamente.<span class="tooltiptext">was to find a pathway to AGI?</span></span>
		<span class="tooltip">En nuestro primer plan
de negocios que distribuimos<span class="tooltiptext">SL: Absolutely.</span></span>
		<span class="tooltip">cuando buscábamos inversores en 2010,<span class="tooltiptext">On our first business plan
that we were circulating</span></span>
		<span class="tooltip">tenía una frase en la portada que decía:<span class="tooltiptext">when we were looking
for investors in 2010,</span></span>
		<span class="tooltip">«Construye la primera inteligencia
artificial general del mundo».<span class="tooltiptext">it had one sentence
on the front cover and it said,</span></span>
		<span class="tooltip">Así que eso fue justo desde el principio.<span class="tooltiptext">&quot;Build the world&#39;s first artificial
general intelligence.&quot;</span></span>
	</p>
	<p>
		<span class="tooltip">CA: ¿Sabías<span class="tooltiptext">So that was right in from the beginning.</span></span>
		<span class="tooltip">que construir esa IAG
podría tener consecuencias<span class="tooltiptext">CA: Even though you knew</span></span>
		<span class="tooltip">apocalípticas en algunos escenarios?<span class="tooltiptext">that building that AGI might actually have</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí.<span class="tooltiptext">apocalyptic consequences
in some scenarios?</span></span>
		<span class="tooltip">Es una tecnología
profundamente transformadora.<span class="tooltiptext">SL: Yeah.</span></span>
		<span class="tooltip">Creo que va a suceder.<span class="tooltiptext">So it&#39;s a deeply
transformative technology.</span></span>
		<span class="tooltip">Pienso que, ya sabes,<span class="tooltiptext">I believe it will happen.</span></span>
		<span class="tooltip">estos algoritmos se pueden entender
y se entenderán en su momento.<span class="tooltiptext">I think that, you know,</span></span>
		<span class="tooltip">Y pienso que la inteligencia
es, fundamentalmente, algo<span class="tooltiptext">these algorithms can be understood
and they will be understood at the time.</span></span>
		<span class="tooltip">increíblemente valioso.<span class="tooltiptext">And I think that intelligence
is fundamentally</span></span>
		<span class="tooltip">Todo lo que nos rodea en este momento,<span class="tooltiptext">an incredibly valuable thing.</span></span>
		<span class="tooltip">el edificio en el que nos encontramos,
las palabras que uso,<span class="tooltiptext">Everything around us at the moment --
the building we’re in,</span></span>
		<span class="tooltip">los conceptos que tenemos,
la tecnología que nos rodea,<span class="tooltiptext">the words I’m using, the concepts we have,
the technology around us --</span></span>
		<span class="tooltip">todas estas cosas se ven afectadas
por la inteligencia.<span class="tooltiptext">you know, all of these things
are being affected by intelligence.</span></span>
		<span class="tooltip">Por lo tanto, desarrollar
inteligencia en las máquinas<span class="tooltiptext">So having intelligence in machines</span></span>
		<span class="tooltip">es algo increíblemente valioso.<span class="tooltiptext">is an incredibly valuable
thing to develop.</span></span>
		<span class="tooltip">Por eso creo que está por venir.<span class="tooltiptext">And so I believe it is coming.</span></span>
	</p>
	<p>
		<span class="tooltip">Ahora, cuando surge una tecnología
muy, muy poderosa,<span class="tooltiptext">Now when a very, very
powerful technology arises,</span></span>
		<span class="tooltip">puede haber una variedad
de resultados diferentes.<span class="tooltiptext">there can be a range
of different outcomes.</span></span>
		<span class="tooltip">Las cosas podrían ir muy, muy bien,<span class="tooltiptext">Things could go very, very well,</span></span>
		<span class="tooltip">pero existe la posibilidad de
que las cosas también vayan mal.<span class="tooltiptext">but there is a possibility things
can go badly as well.</span></span>
		<span class="tooltip">Y eso era algo de lo que también
era consciente desde hace unos 20 años.<span class="tooltiptext">And that was something I was aware of
also from about 20 years ago.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: A medida que DeepMind se desarrollaba,<span class="tooltiptext">CA: So talk about, as DeepMind developed,</span></span>
		<span class="tooltip">¿hubo un momento 
en el que realmente pensaste<span class="tooltiptext">was there a moment where you really felt,</span></span>
		<span class="tooltip">«Dios mío, estamos ante algo
increíblemente poderoso»?<span class="tooltiptext">&quot;My goodness, we&#39;re onto
something unbelievably powerful?&quot;</span></span>
		<span class="tooltip">Por ejemplo, ¿fue AlphaGo, toda esa
historia o cuál fue el momento para ti?<span class="tooltiptext">Like, was it AlphaGo, that whole story,
or what was the moment for you?</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí, hubo muchos momentos
a lo largo de los años.<span class="tooltiptext">SL: Yeah, there were many
moments over the years.</span></span>
		<span class="tooltip">Uno fue cuando hicimos
los juegos de Atari.<span class="tooltiptext">One was when we did the Atari games.</span></span>
		<span class="tooltip">¿Has visto esos vídeos en los
que teníamos un algoritmo<span class="tooltiptext">Have you seen those videos</span></span>
		<span class="tooltip">que podía aprender a jugar a varios juegos<span class="tooltiptext">where we had an algorithm
that could learn to play multiple games</span></span>
		<span class="tooltip">sin estar programado
para ningún juego específico?<span class="tooltiptext">without being programmed
for any specific game?</span></span>
		<span class="tooltip">Allí hubo algunos momentos emocionantes.<span class="tooltiptext">There were some exciting moments there.</span></span>
		<span class="tooltip">Go, por supuesto, fue
un momento muy emocionante.<span class="tooltiptext">Go, of course, was a really
exciting moment.</span></span>
	</p>
	<p>
		<span class="tooltip">Pero creo que lo que realmente
ha capturado mi imaginación,<span class="tooltiptext">But I think the thing that&#39;s really
captured my imagination,</span></span>
		<span class="tooltip">la imaginación de mucha gente,<span class="tooltiptext">a lot of people&#39;s imagination,</span></span>
		<span class="tooltip">es la extraordinaria expansión
de los modelos lingüísticos<span class="tooltiptext">is the phenomenal scaling
of language models in recent years.</span></span>
		<span class="tooltip">en los últimos años.<span class="tooltiptext">I think we can see they&#39;re systems</span></span>
		<span class="tooltip">Creo que podemos ver que son sistemas<span class="tooltiptext">that really can start
to do some meaningful fraction</span></span>
		<span class="tooltip">que realmente pueden empezar
a realizar una fracción significativa<span class="tooltiptext">of the cognitive tasks that people can do.</span></span>
		<span class="tooltip">de las tareas cognitivas que
las personas pueden realizar.<span class="tooltiptext">CA: Now, you were working on those models,</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Estabas trabajando
en esos modelos,<span class="tooltiptext">but were you, to some extent, blindsided</span></span>
		<span class="tooltip">¿te sorprendió hasta cierto punto
la repentina revelación de ChatGPT<span class="tooltiptext">by OpenAI&#39;s, sort of,
sudden unveiling of ChatGPT?</span></span>
		<span class="tooltip">por parte de OpenAI?<span class="tooltiptext">SL: Right.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Correcto.<span class="tooltiptext">We were working on them and you know,</span></span>
		<span class="tooltip">Estábamos trabajando en ellos<span class="tooltiptext">the transformer model
was invented in Google,</span></span>
		<span class="tooltip">y, el modelo transformador
se inventó en Google,<span class="tooltiptext">and we had teams who were building big
transformer language models and so on.</span></span>
		<span class="tooltip">y teníamos equipos que creaban grandes
modelos de lenguaje transformador, etc.<span class="tooltiptext">CA: Google acquired DeepMind
at some point in this journey.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Google adquirió DeepMind
en algún momento de este viaje.<span class="tooltiptext">SL: Yeah, exactly.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí, exactamente.<span class="tooltiptext">And so what I didn&#39;t expect</span></span>
		<span class="tooltip">Así que lo que no me esperaba<span class="tooltiptext">was just how good a model could get
training purely on text.</span></span>
		<span class="tooltip">era la calidad con la que un modelo
podía formarse únicamente con textos.<span class="tooltiptext">I thought you would
need more multimodality.</span></span>
		<span class="tooltip">Pensé que necesitarías
más multimodalidad.<span class="tooltiptext">You&#39;d need images, you&#39;d need sound,
you&#39;d need video and things like that.</span></span>
		<span class="tooltip">Necesitarías imágenes,
sonido, vídeo y cosas así.<span class="tooltiptext">But due to the absolutely
vast quantities of text,</span></span>
		<span class="tooltip">Sin embargo, las enormes 
cantidades de texto<span class="tooltiptext">it can sort of compensate
for these things to an extent.</span></span>
		<span class="tooltip">en cierto modo pueden compensar
estas cosas hasta cierto punto.<span class="tooltiptext">I still think you see aspects of this.</span></span>
		<span class="tooltip">Sigo pensando que ves 
algunos aspectos de esto.<span class="tooltiptext">I think language models
tend to be weak in areas</span></span>
		<span class="tooltip">Creo que los modelos lingüísticos
tienden a ser débiles en áreas<span class="tooltiptext">that are not easily expressed in text.</span></span>
		<span class="tooltip">que no se expresan fácilmente en el texto.<span class="tooltiptext">But I don’t think
this is a fundamental limitation.</span></span>
		<span class="tooltip">Pero no creo que esta sea
una limitación fundamental.<span class="tooltiptext">I think we&#39;re going to see these language
models expanding into video</span></span>
		<span class="tooltip">Veremos cómo estos modelos lingüísticos
se expanden al vídeo, las imágenes<span class="tooltiptext">and images and sound and all these things,</span></span>
		<span class="tooltip">y el sonido y todas esas cosas,<span class="tooltiptext">and these things will be overcome in time.</span></span>
		<span class="tooltip">y estas cosas se superarán con el tiempo.<span class="tooltiptext">CA: So talk to us, Shane,</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Háblanos, Shane,<span class="tooltiptext">about the things that you, at this moment,</span></span>
		<span class="tooltip">sobre las cosas en las que,
en este momento, sientes<span class="tooltiptext">passionately feel that the world needs
to be thinking about more cogently.</span></span>
		<span class="tooltip">apasionadamente que el mundo necesita
pensar de manera más convincente.<span class="tooltiptext">SL: Right.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Correcto.<span class="tooltiptext">So I think that very, very powerful,</span></span>
		<span class="tooltip">pienso que está en camino
una inteligencia artificial<span class="tooltiptext">very intelligent artificial
intelligence is coming.</span></span>
		<span class="tooltip">muy, muy poderosa e inteligente.<span class="tooltiptext">I think that this is very, very likely.</span></span>
		<span class="tooltip">Pienso que esto es muy, muy probable.<span class="tooltiptext">I don&#39;t think it&#39;s coming today.</span></span>
		<span class="tooltip">Pienso que no llegará hoy.<span class="tooltiptext">I don&#39;t think it&#39;s coming next
year or the year after.</span></span>
		<span class="tooltip">No creo que llegue el año
que viene ni el siguiente.<span class="tooltiptext">It&#39;s probably a little bit
further out than that.</span></span>
		<span class="tooltip">Probablemente esté un poco
más lejos que eso.<span class="tooltiptext">CA: 2028?</span></span>
	</p>
	<p>
		<span class="tooltip">CA: ¿2028?<span class="tooltiptext">SL: 2028, that&#39;s a 50 percent chance.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: 2028, esa es una probabilidad
del 50 %.<span class="tooltiptext">So, you know, if it doesn&#39;t
happen in 2028,</span></span>
		<span class="tooltip">Así que, si no ocurre en 2028, 
obviamente no me sorprenderá.<span class="tooltiptext">I&#39;m not going to be surprised, obviously.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Y cuando dices poderosa,<span class="tooltiptext">CA: And when you say powerful,</span></span>
	</p>
	<p>
		<span class="tooltip">quiero decir que ya
existe una IA poderosa.<span class="tooltiptext">I mean there&#39;s already
powerful AI out there.</span></span>
		<span class="tooltip">Básicamente estás diciendo
que viene una versión<span class="tooltiptext">But you&#39;re saying basically a version</span></span>
		<span class="tooltip">de inteligencia artificial general.<span class="tooltiptext">of artificial general
intelligence is coming.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí.<span class="tooltiptext">SL: Yeah.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Danos una idea de cómo podría verse.<span class="tooltiptext">CA: So give us a picture
of what that could look like.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Bueno, una inteligencia
artificial general<span class="tooltiptext">SL: Well, if you had
an artificial general intelligence,</span></span>
		<span class="tooltip">podría hacer todo tipo 
de cosas increíbles.<span class="tooltiptext">you could do all sorts of amazing things.</span></span>
		<span class="tooltip">Al igual que la inteligencia humana
puede hacer muchísimas cosas asombrosas.<span class="tooltiptext">Just like human intelligence is able
to do many, many amazing things.</span></span>
		<span class="tooltip">No es algo específico,<span class="tooltiptext">So it&#39;s not really about a specific thing,</span></span>
		<span class="tooltip">ese es el objetivo de la generalidad.<span class="tooltiptext">that&#39;s the whole point of the generality.</span></span>
	</p>
	<p>
		<span class="tooltip">Pero para darles un ejemplo,<span class="tooltiptext">But to give you one example,</span></span>
		<span class="tooltip">desarrollamos el sistema AlphaFold,<span class="tooltiptext">we developed the system AlphaFold,</span></span>
		<span class="tooltip">que tomará una proteína y calculará,
básicamente, la forma de esa proteína.<span class="tooltiptext">which will take a protein and compute,
basically, the shape of that protein.</span></span>
		<span class="tooltip">Y eso permite realizar 
todo tipo de investigaciones<span class="tooltiptext">And that enables you to do
all sorts of research</span></span>
		<span class="tooltip">para comprender los procesos biológicos,<span class="tooltiptext">into understanding biological processes,</span></span>
		<span class="tooltip">desarrollar medicamentos 
y cosas por el estilo.<span class="tooltiptext">developing medicines
and all kinds of things like that.</span></span>
		<span class="tooltip">Ahora bien, si tuvieras una IAG,<span class="tooltiptext">Now, if you had an AGI system,</span></span>
		<span class="tooltip">en lugar de necesitar
lo que teníamos en DeepMind,<span class="tooltiptext">instead of requiring
what we had at DeepMind,</span></span>
		<span class="tooltip">unos 30 científicos de talla mundial<span class="tooltiptext">about roughly 30 world-class scientists</span></span>
		<span class="tooltip">trabajando durante unos
tres años para desarrollarlo,<span class="tooltiptext">working for about three years
to develop that,</span></span>
		<span class="tooltip">tal vez podrías desarrollarlo
con solo un equipo<span class="tooltiptext">maybe you could develop that
with just a team</span></span>
		<span class="tooltip">de un puñado de científicos en un año.<span class="tooltiptext">of a handful of scientists in one year.</span></span>
		<span class="tooltip">Imagina estos desarrollos, 
algo así como los de Alphafold,<span class="tooltiptext">So imagine these, sort of,
AlphaFold-level developments</span></span>
		<span class="tooltip">que se llevan a cabo en todo
el mundo de forma regular.<span class="tooltiptext">taking place around the world
on a regular basis.</span></span>
		<span class="tooltip">Este es el tipo de cosas
que la IAG podría permitir.<span class="tooltiptext">This is the sort of thing
that AGI could enable.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Unos meses después de que la IAG
esté con nosotros, por así decirlo,<span class="tooltiptext">CA: So within months of AGI
being with us, so to speak,</span></span>
		<span class="tooltip">es muy posible que algunos
de los desafíos científicos a los que<span class="tooltiptext">it&#39;s quite possible that some
of the scientific challenges</span></span>
		<span class="tooltip">los humanos se han enfrentado
durante décadas, siglos, por así decirlo,<span class="tooltiptext">that humans have wrestled with
for decades, centuries, if you like,</span></span>
		<span class="tooltip">comiencen a ir surgiendo
en rápida sucesión.<span class="tooltiptext">will start to tumble in rapid succession.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Sí, creo que abrirá todo
tipo de posibilidades asombrosas.<span class="tooltiptext">SL: Yeah, I think it&#39;ll open up
all sorts of amazing possibilities.</span></span>
		<span class="tooltip">En realidad, podría tratarse
de una época dorada de la humanidad,<span class="tooltiptext">And it could be really
a golden age of humanity</span></span>
		<span class="tooltip">en la que la inteligencia humana,<span class="tooltiptext">where human intelligence,</span></span>
		<span class="tooltip">que se ve reforzada 
por la inteligencia artificial,<span class="tooltiptext">which is aided and extended
with machine intelligence,</span></span>
		<span class="tooltip">nos permita hacer todo
tipo de cosas fantásticas<span class="tooltiptext">enables us to do all sorts
of fantastic things</span></span>
		<span class="tooltip">y resolver problemas que
antes eran simplemente insolubles.<span class="tooltiptext">and solve problems that previously
were just intractable.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Así que volvamos a eso.<span class="tooltiptext">CA: So let&#39;s come back to that.</span></span>
		<span class="tooltip">Pero pienso que además,
no eres solo un optimista irremediable,<span class="tooltiptext">But I think you also,</span></span>
		<span class="tooltip">sino que ves la posibilidad
de que las cosas vayan muy mal<span class="tooltiptext">you&#39;re not like,
an irredeemable optimist only,</span></span>
		<span class="tooltip">en una dirección diferente.<span class="tooltiptext">you see a potential for it to go
very badly in a different direction.</span></span>
		<span class="tooltip">Hable sobre cómo podría ser ese camino.<span class="tooltiptext">Talk about what that pathway
could look like.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Bueno, sí, quiero explicarlo.<span class="tooltiptext">SL: Well, yeah, I want to explain.</span></span>
		<span class="tooltip">No le creo a la gente<span class="tooltiptext">I don&#39;t believe the people</span></span>
		<span class="tooltip">que está segura de que
le va a ir muy bien,<span class="tooltiptext">who are sure that it&#39;s going
to go very well,</span></span>
		<span class="tooltip">y no le creo a la gente<span class="tooltiptext">and I don&#39;t believe the people</span></span>
		<span class="tooltip">que está segura de que
le va a ir muy, muy mal.<span class="tooltiptext">who are sure that it’s going
to go very, very badly.</span></span>
		<span class="tooltip">Porque estamos hablando de una transición
increíblemente profunda.<span class="tooltiptext">Because what we’re talking about
is an incredibly profound transition.</span></span>
		<span class="tooltip">Es como la llegada de
la inteligencia humana al mundo.<span class="tooltiptext">It&#39;s like the arrival of human
intelligence in the world.</span></span>
		<span class="tooltip">Esta es otra inteligencia
que llega al mundo.<span class="tooltiptext">This is another intelligence
arriving in the world.</span></span>
		<span class="tooltip">Por lo tanto, se trata de una transición
increíblemente profunda,<span class="tooltiptext">And so it is an incredibly
deep transition,</span></span>
		<span class="tooltip">y no entendemos completamente
todas las implicaciones<span class="tooltiptext">and we do not fully understand
all the implications</span></span>
		<span class="tooltip">y consecuencias de esto.<span class="tooltiptext">and consequences of this.</span></span>
		<span class="tooltip">Así que no podemos estar seguros
de que vaya a ser esto, aquello o lo otro.<span class="tooltiptext">And so we can&#39;t be certain</span></span>
		<span class="tooltip">Tenemos que tener la mente abierta
en cuanto a lo que pueda suceder.<span class="tooltiptext">that it&#39;s going to be this,
that or the other thing.</span></span>
	</p>
	<p>
		<span class="tooltip">Tengo cierto optimismo porque creo
que si quieres que un sistema sea seguro,<span class="tooltiptext">So we have to be open-minded
about what may happen.</span></span>
		<span class="tooltip">tienes que entender
mucho sobre ese sistema.<span class="tooltiptext">I have some optimism because I think</span></span>
		<span class="tooltip">No puedes hacer que un avión sea seguro<span class="tooltiptext">that if you want to make a system safe,</span></span>
		<span class="tooltip">si no sabes cómo funcionan los aviones.<span class="tooltiptext">you need to understand
a lot about that system.</span></span>
		<span class="tooltip">Así pues, a medida que nos
acerquemos a la IAG,<span class="tooltiptext">You can&#39;t make an airplane safe</span></span>
		<span class="tooltip">comprenderemos más y más
acerca de estos sistemas<span class="tooltiptext">if you don&#39;t know
about how airplanes work.</span></span>
		<span class="tooltip">y veremos más formas de hacer
que estos sistemas sean seguros<span class="tooltiptext">So as we get closer to AGI,</span></span>
		<span class="tooltip">y de crear sistemas de inteligencia
artificial altamente éticos.<span class="tooltiptext">we will understand more
and more about these systems,</span></span>
	</p>
	<p>
		<span class="tooltip">Sin embargo, hay muchas cosas
que no entendemos sobre el futuro.<span class="tooltiptext">and we will see more ways
to make these systems safe,</span></span>
		<span class="tooltip">Así que tengo que aceptar que existe
la posibilidad de que las cosas vayan mal<span class="tooltiptext">make highly ethical AI systems.</span></span>
		<span class="tooltip">porque no sé qué va a pasar.<span class="tooltiptext">But there are many things
we don&#39;t understand about the future.</span></span>
		<span class="tooltip">No puedo saber eso del futuro
con un cambio tan grande.<span class="tooltiptext">So I have to accept that there is
a possibility that things may go badly</span></span>
		<span class="tooltip">E incluso si la probabilidad de que
algo vaya mal es muy pequeña,<span class="tooltiptext">because I don&#39;t know
what&#39;s going to happen.</span></span>
		<span class="tooltip">debemos tomárnoslo muy en serio.<span class="tooltiptext">I can&#39;t know that about the future
in such a big change.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Pinta un escenario de cómo
podría ser que saliera mal.<span class="tooltiptext">And even if the probability of something
going bad is quite small,</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Bueno, es difícil de hacer<span class="tooltiptext">we should take this extremely seriously.</span></span>
		<span class="tooltip">porque estamos hablando de sistemas<span class="tooltiptext">CA: Paint a scenario
of what going bad could look like.</span></span>
		<span class="tooltip">que potencialmente tienen
una inteligencia sobrehumana, ¿verdad?<span class="tooltiptext">SL: Well, it&#39;s hard to do</span></span>
		<span class="tooltip">Así que hay muchas maneras en las que
las cosas podrían ir mal en el mundo.<span class="tooltiptext">because you&#39;re talking about systems</span></span>
		<span class="tooltip">La gente a veces señala, no sé,
patógenos modificados genéticamente, ¿sí?<span class="tooltiptext">that potentially have superhuman
intelligence, right?</span></span>
		<span class="tooltip">Quizá una superinteligencia podría diseñar
un patógeno modificado genéticamente.<span class="tooltiptext">So there are many ways in which
things would go bad in the world.</span></span>
		<span class="tooltip">Podrían ser cosas mucho más mundanas.<span class="tooltiptext">People sometimes point to, I don&#39;t know,
engineered pathogens, right?</span></span>
		<span class="tooltip">Puede que con el AGI,<span class="tooltiptext">Maybe a superintelligence could
design an engineered pathogen.</span></span>
		<span class="tooltip">ya sabes, se utilice para desestabilizar
la democracia en el mundo,<span class="tooltiptext">It could be much more mundane things.</span></span>
		<span class="tooltip">con, ya sabes, propaganda
o todo tipo de cosas así.<span class="tooltiptext">Maybe with AGI, you know,</span></span>
		<span class="tooltip">No lo sabemos...<span class="tooltiptext">it gets used to destabilize
democracy in the world,</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Es posible que eso ya haya ocurrido.<span class="tooltiptext">with, you know, propaganda
or all sorts of other things like that.</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Puede que ya esté ocurriendo un poco.<span class="tooltiptext">We don&#39;t know --</span></span>
		<span class="tooltip">Pero, ya sabes, puede haber
mucho más de esto<span class="tooltiptext">CA: That one might already have happened.</span></span>
		<span class="tooltip">si tenemos sistemas más potentes.<span class="tooltiptext">SL: There might be happening
a bit already.</span></span>
		<span class="tooltip">Hay muchas maneras
de desestabilizar sociedades.<span class="tooltiptext">But, you know, there may
be a lot more of this</span></span>
		<span class="tooltip">Se puede ver en los libros de historia.<span class="tooltiptext">if we have more powerful systems.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Shane, si se le hubiera podido
preguntar a todos los humanos,<span class="tooltiptext">So there are many ways in which
societies can be destabilized.</span></span>
		<span class="tooltip">hace 15 años, OK, 
podemos abrir una puerta aquí,<span class="tooltiptext">And you can see that in the history books.</span></span>
		<span class="tooltip">y abrir esta puerta podría llevar
a los mejores resultados<span class="tooltiptext">CA: I mean, Shane, if you
could have asked all humans,</span></span>
		<span class="tooltip">de la historia para la humanidad.<span class="tooltiptext">say, 15 years ago, OK,
we can open a door here,</span></span>
		<span class="tooltip">Pero también hay una posibilidad,
digamos que de más del 5 %,<span class="tooltiptext">and opening this door could lead
to the best-ever outcomes for humanity.</span></span>
		<span class="tooltip">de que podamos
destruir nuestra civilización.<span class="tooltiptext">But there&#39;s also a meaningful chance,</span></span>
		<span class="tooltip">¿no hay posibilidad de que la mayoría
de la gente hubiera dicho realmente:<span class="tooltiptext">let&#39;s say it&#39;s more than five percent,</span></span>
		<span class="tooltip">«No te atrevas a abrir 
esa maldita puerta».<span class="tooltiptext">that we could actually
destroy our civilization.</span></span>
		<span class="tooltip">Esperemos».<span class="tooltiptext">I mean, isn&#39;t there a chance
that most people would have actually said,</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Si tuviera una varita mágica
y pudiera ralentizar las cosas,<span class="tooltiptext">&quot;Don&#39;t you dare open that damn door.</span></span>
		<span class="tooltip">la usaría, pero no la uso.<span class="tooltiptext">Let&#39;s wait.&quot;</span></span>
		<span class="tooltip">Hay docenas de compañías,<span class="tooltiptext">SL: If I had a magic wand
and I could slow things down,</span></span>
		<span class="tooltip">bueno, probablemente haya
10 compañías en el mundo<span class="tooltiptext">I would use that magic wand, but I don&#39;t.</span></span>
		<span class="tooltip">que pueden desarrollar los modelos
más vanguardistas, incluyendo, pienso,<span class="tooltiptext">There are dozens of companies,</span></span>
		<span class="tooltip">algunos servicios
de inteligencia nacionales<span class="tooltiptext">well, there&#39;s probably
10 companies in the world now</span></span>
		<span class="tooltip">que tienen proyectos secretos
en este sentido.<span class="tooltiptext">that can develop the most
cutting-edge models, including, I think,</span></span>
		<span class="tooltip">Y luego hay, no sé,<span class="tooltiptext">some national intelligence services
who have secret projects doing this.</span></span>
		<span class="tooltip">decenas de empresas que pueden desarrollar
algo que lleva una generación de retraso.<span class="tooltiptext">And then there&#39;s, I don&#39;t know,</span></span>
		<span class="tooltip">Y recuerda que la inteligencia
es increíblemente valiosa.<span class="tooltiptext">dozens of companies that can develop
something that&#39;s a generation behind.</span></span>
		<span class="tooltip">Es increíblemente útil.<span class="tooltiptext">And remember, intelligence
is incredibly valuable.</span></span>
		<span class="tooltip">Hacemos esto<span class="tooltiptext">It&#39;s incredibly useful.</span></span>
		<span class="tooltip">porque podemos ver todo tipo de
valor que se puede crear en esto<span class="tooltiptext">We&#39;re doing this</span></span>
		<span class="tooltip">por todo tipo de razones.<span class="tooltiptext">because we can see all kinds of value
that can be created in this</span></span>
		<span class="tooltip">¿Cómo se detiene este proceso?<span class="tooltiptext">for all sorts of reasons.</span></span>
		<span class="tooltip">No veo ningún plan realista
del que haya oído hablar<span class="tooltiptext">How do you stop this process?</span></span>
		<span class="tooltip">para detener este proceso<span class="tooltiptext">I don&#39;t see any realistic plan
that I&#39;ve heard of,</span></span>
		<span class="tooltip">Tal vez podamos...<span class="tooltiptext">of stopping this process.</span></span>
		<span class="tooltip">deberíamos pensar en regular las cosas.<span class="tooltiptext">Maybe we can --</span></span>
		<span class="tooltip">Regulaciones<span class="tooltiptext">I think we should think
about regulating things.</span></span>
		<span class="tooltip">como en todas las tecnologías potentes.<span class="tooltiptext">I think we should do things like this
as we do with every powerful technology.</span></span>
		<span class="tooltip">En este caso, la IA 
no tiene nada de especial.<span class="tooltiptext">There&#39;s nothing special about AI here.</span></span>
		<span class="tooltip">La gente dice ¿cómo te atreves
a hablar de regular esto?<span class="tooltiptext">People talk about, oh, you know,
how dare you talk about regulating this?</span></span>
		<span class="tooltip">Regulamos tecnologías poderosas todo
el tiempo en beneficio de la sociedad.<span class="tooltiptext">No, we regulate powerful technologies
all the time in the interests of society.</span></span>
		<span class="tooltip">Es algo muy importante
que deberíamos analizar.<span class="tooltiptext">And I think this is a very important
thing that we should be looking at.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Más o menos es la primera vez que
tenemos esta tecnología superpoderosa<span class="tooltiptext">CA: It&#39;s kind of the first time we have
this superpowerful technology out there</span></span>
		<span class="tooltip">y, literalmente, no entendemos
completamente cómo funciona.<span class="tooltiptext">that we literally don&#39;t understand
in full how it works.</span></span>
		<span class="tooltip">En tu opinión, lo más importante
que debemos hacer es entender,<span class="tooltiptext">Is the most single, most important thing
we must do, in your view,</span></span>
		<span class="tooltip">entender mejor lo que sucede en la Tierra,<span class="tooltiptext">to understand, to understand better
what on Earth is going on,</span></span>
		<span class="tooltip">para tener la menor posibilidad
de orientarlo en la dirección correcta.<span class="tooltiptext">so that we least have a shot
at pointing it in the right direction?</span></span>
	</p>
	<p>
		<span class="tooltip">SL: Hay mucha energía detrás de
las capacidades en este momento,<span class="tooltiptext">SL: There is a lot of energy
behind capabilities at the moment</span></span>
		<span class="tooltip">porque desarrollar las capacidades
tiene mucho valor.<span class="tooltiptext">because there&#39;s a lot of value
in developing the capabilities.</span></span>
		<span class="tooltip">Creo que necesitamos dedicar
mucha más energía a la ciencia real,<span class="tooltiptext">I think we need to see a lot more energy
going into actual science,</span></span>
		<span class="tooltip">entender cómo funcionan estas redes,<span class="tooltiptext">understanding how these networks work,</span></span>
		<span class="tooltip">qué están haciendo, qué no están haciendo,<span class="tooltiptext">what they&#39;re doing,
what they&#39;re not doing,</span></span>
		<span class="tooltip">dónde es probable que fallen,<span class="tooltiptext">where they&#39;re likely to fail,</span></span>
		<span class="tooltip">y entender cómo combinamos estas cosas<span class="tooltiptext">and understanding how we put
these things together</span></span>
		<span class="tooltip">para poder encontrar formas
de hacer que estos sistemas de IAG<span class="tooltiptext">so that we&#39;re able to find ways</span></span>
		<span class="tooltip">sean profundamente éticos y seguros.<span class="tooltiptext">to make these AGI systems
profoundly ethical and safe.</span></span>
	</p>
	<p>
		<span class="tooltip">Creo que es posible.<span class="tooltiptext">I believe it&#39;s possible.</span></span>
		<span class="tooltip">Pero necesitamos movilizar la mente
y el cerebro de más personas<span class="tooltiptext">But we need to mobilize
more people&#39;s minds and brains</span></span>
		<span class="tooltip">para encontrar estas soluciones,<span class="tooltiptext">to finding these solutions</span></span>
		<span class="tooltip">de modo que acabemos en un futuro<span class="tooltiptext">so that we end up in a future</span></span>
		<span class="tooltip">en el que tengamos una IA
increíblemente poderosa,<span class="tooltiptext">where we have incredibly
powerful machine intelligence</span></span>
		<span class="tooltip">que también sea profundamente ética 
y segura, y que mejore<span class="tooltiptext">that&#39;s also profoundly ethical and safe,</span></span>
		<span class="tooltip">y extienda a la humanidad<span class="tooltiptext">and it enhances and extends humanity</span></span>
		<span class="tooltip">hasta convertirla, en mi opinión, en un
nuevo período dorado para la humanidad.<span class="tooltiptext">into, I think, a new
golden period for humanity.</span></span>
	</p>
	<p>
		<span class="tooltip">CA: Shane, muchísimas gracias
por compartir esa visión<span class="tooltiptext">CA: Shane, thank you so much
for sharing that vision</span></span>
		<span class="tooltip">y venir a TED.<span class="tooltiptext">and coming to TED.</span></span>
	</p>
	<p>
		<span class="tooltip">(Aplausos)<span class="tooltiptext">(Applause)</span></span>
	</p>

	<div class="line"></div>
	<p>Enjoying my website? Your donation helps maintain its ad-free experience and smooth operation. Your support is truly appreciated and helps me continue providing quality content. Thank you for considering a donation!</p>
	<p>To donate, simply click the PayPal link below:<p/>
	<p><a href="https://www.paypal.com/paypalme/otechai" target='_blank'>Donate via PayPal</a></p>
	
</div>
<span onclick='topFunction()' id='gotop' title='Go to top'><svg width="32" height="32" viewBox="0 0 100 100">
	   <path fill="white" d="m50 0c-13.262 0-25.98 5.2695-35.355 14.645s-14.645 22.094-14.645 35.355 5.2695 25.98 14.645 35.355 22.094 14.645 35.355 14.645 25.98-5.2695 35.355-14.645 14.645-22.094 14.645-35.355-5.2695-25.98-14.645-35.355-22.094-14.645-35.355-14.645zm20.832 62.5-20.832-22.457-20.625 22.457c-1.207 0.74219-2.7656 0.57812-3.7891-0.39844-1.0273-0.98047-1.2695-2.5273-0.58594-3.7695l22.918-25c0.60156-0.61328 1.4297-0.96094 2.2891-0.96094 0.86328 0 1.6914 0.34766 2.293 0.96094l22.918 25c0.88672 1.2891 0.6875 3.0352-0.47266 4.0898-1.1562 1.0508-2.9141 1.0859-4.1133 0.078125z"></path>
	 </svg></span>

	    <div class="footer">
	        <div class="left"><a href='../../../index.html'>ReadingDB</a></div>
	        <div class="center"><a href="https://twitter.com/readingdb" class="twitter-link">
	    <svg class="twitter-icon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
	        <path d="M22.46 6c-.85.38-1.78.64-2.75.76 1-.6 1.76-1.55 2.12-2.68-.93.55-1.96.95-3.06 1.17-.88-.94-2.13-1.53-3.51-1.53-2.66 0-4.81 2.16-4.81 4.81 0 .38.04.75.13 1.1-4-.2-7.58-2.11-9.96-5.02-.42.72-.66 1.56-.66 2.46 0 1.68.85 3.16 2.14 4.02-.79-.02-1.53-.24-2.18-.6v.06c0 2.35 1.67 4.31 3.88 4.76-.4.1-.83.16-1.27.16-.31 0-.62-.03-.92-.08.63 1.96 2.45 3.39 4.61 3.43-1.69 1.32-3.83 2.1-6.15 2.1-.4 0-.8-.02-1.19-.07 2.19 1.4 4.78 2.22 7.57 2.22 9.07 0 14.02-7.52 14.02-14.02 0-.21 0-.41-.01-.61.96-.69 1.79-1.56 2.45-2.55-.88.39-1.83.65-2.82.77z"/>
	    </svg>
	</a></div>
	        <div class="right"><a href='https://www.paypal.com/paypalme/otechai'>Donate</a></div>
	    </div>
	<script>

		var mybutton = document.getElementById('gotop');
		window.onscroll = function() {scrollFunction()};
		function scrollFunction() {
			if (document.documentElement.scrollTop > 20 || document.body.scrollTop > 20) {
				mybutton.style.display = 'block';
			} else {
				mybutton.style.display = 'none';
			}
		}
		function topFunction() {
			document.body.scrollTop = 0;
			document.documentElement.scrollTop = 0;
		}
	</script>
	<script src="../../../static/scripts/script.js"></script>
</body>
</html>
	